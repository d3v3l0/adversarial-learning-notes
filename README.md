# Adversarial Learning Notes
Presonal collection of resources on adversarial learning.

In a machine learning model, adversarial inputs are generated by updating inputs in a way that it improves the score for the desired output while fixing the model parameters.

## Introduction
* [Breaking Linear Classifiers on ImageNet](http://karpathy.github.io/2015/03/30/breaking-convnets) by Andrej Karpathy :white_check_mark:
* [Is Attacking Machine Learning Easier Than Defending It?](http://www.cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html) by Ian Goodfellow and Nicolas Papernot :white_check_mark:
  * Two Defense Techniques: aversarial training and defensive distillation
  * Gradient masking for defense doesn't work
  * Adversarial learning issue exists when small perturbation to inputs doesn't change desired outcomes.
* [The Challenge of Verification and Testing of Machine Learning](http://www.cleverhans.io/security/privacy/ml/2017/06/14/verification.html) by Ian Goodfellow and Nicolas Papernot
* [Attacking Machine Learning with Adversarial Examples](https://blog.openai.com/adversarial-example-research/) by Ian Goodfellow et al.
* [Introduction to Adversarial Machine Learning](https://mascherari.press/introduction-to-adversarial-machine-learning/) by Sarah Jamie Lewis

## Papers
* [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) by Ian Goodfellow et al. at arXiv
* [Distillation as a Defense to Adversarial Perturbations against Deep Neural Network](https://arxiv.org/abs/1511.04508) by Nicolas Papernot et al. at arXiv
* [Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) by Cynthia Dwork and Aaron Roth at Foundations and Trends‚Éù in Theoretical Computer Science

## Code
* [CleverHans](https://github.com/tensorflow/cleverhans) - Python library to benchmark ML systems' vulnerability to adversarial examples. It uses TensorFlow.

## Tutorials
* [MNIST Tutorial: fast gradient sign method and adversarial training](https://github.com/tensorflow/cleverhans/blob/master/tutorials/mnist_tutorial_keras_tf.md) at CleverHans 
* [MNIST tutorial: crafting adversarial examples with the Jacobian-based saliency map attack](https://github.com/tensorflow/cleverhans/blob/master/tutorials/mnist_tutorial_jsma.md) at CleverHans

## Competitions
* [NIPS 2017 Competition Track - Competitionson Adversarial Attacks and Defenses](https://nips.cc/Conferences/2017/CompetitionTrack)
  * [Non-targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack) - The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
  * [Targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack) - The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
  * [Defense Against Adversarial Attack](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack) - The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.

## References
* [Awesome Adversarial Machine Learning](https://github.com/yenchenlin/awesome-adversarial-machine-learning) by Yen-Chen Lin
